{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be4ff72-208a-49b8-8010-9602adb0c634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: joblib in c:\\python\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn joblib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337571be-be2b-4dbe-829a-ed2f4de5ce2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\python\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\python\\lib\\site-packages (from xgboost) (1.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c9002a-9580-43b0-9468-ebce85f6b7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python\\lib\\site-packages (2.5.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torchvision in c:\\python\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: torchaudio in c:\\python\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lalitha priya.a\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\python\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\python\\lib\\site-packages (from torch) (73.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54009db-1235-438a-9cb9-bf6a3e96dd92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: xgboost in c:\\python\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: joblib in c:\\python\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn xgboost joblib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb9718c-c1d4-4db3-8dcf-db130e84d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7e4a75-9c8c-4f82-b6f7-fb9965009a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('trainingdata_with_ps_50k.csv')\n",
    "df = df.drop(['Weather_Severity'], axis=1)\n",
    "df = pd.get_dummies(df, columns=['Weather_Condition'])#converts it into binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0082ac6-2ad3-441a-904b-5a11dac0baed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df.drop(['Base_Priority', 'Business_Adjusted_Priority', 'Final_Priority', 'Normalized_Priority_Score'], axis=1)#features\n",
    "y = df['Normalized_Priority_Score']#target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e84c65-ecde-4081-8b0b-f57b84065592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65059fdd-03e8-4eda-b8cb-6e51de193ed1",
   "metadata": {},
   "source": [
    "Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba5f0ed-c9ca-49f6-a61b-a7bfb6cb1e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34752cba",
   "metadata": {},
   "source": [
    "Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe214e9f-5f16-4f21-b124-4daa8329d89b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Percentage Error: 2.4760042455158495\n",
      "XGBoost model saved as 'xgb_model.joblib'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_gs, X_val, y_train_gs, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "xgb_reg = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',\n",
    "    max_depth=6,                  \n",
    "    eta=0.05,                     \n",
    "    subsample=0.9,                \n",
    "    colsample_bytree=0.9,         \n",
    "    reg_alpha=0.1,                \n",
    "    reg_lambda=1.0,               \n",
    "    n_estimators=500              \n",
    ")\n",
    "\n",
    "# Train with early stopping to monitor validation error\n",
    "xgb_reg.fit(\n",
    "    X_train_gs, y_train_gs,\n",
    "    eval_set=[(X_val, y_val)],#early stopping\n",
    "       \n",
    "    verbose=False\n",
    ")\n",
    "results = xgb_reg.evals_result()\n",
    "\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "percentage_error = 100 * np.abs((y_val - y_pred) / y_val)\n",
    "mean_percentage_error = np.nanmean(percentage_error)\n",
    "\n",
    "\n",
    "print(f\"Mean Percentage Error: {mean_percentage_error}\")\n",
    "joblib.dump(xgb_reg, 'xgb_model.joblib')\n",
    "print(\"XGBoost model saved as 'xgb_model.joblib'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c42c7-b63b-4df3-ab9e-a184c4ba90f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a778b8-5e92-4a72-a2d6-2b889770b325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE: 0.02641895926910324\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_val_score(xgb_reg, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Cross-Validation MSE:\", -cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86296d5f-5b24-4aa5-9ed4-3f9b2260b5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_xgb_model = joblib.load('xgb_model.joblib')\n",
    "y_pred_xgb = loaded_xgb_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b0eeed7-ff8a-4dde-815a-1954d5fa6b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_reg.save_model('xgb_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7988066-51fc-4e77-99a1-5b37324046a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Evaluate the xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b139bc27-d9ad-443a-a51b-ae133cf1fd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse_xgb = root_mean_squared_error(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f32cbc-71c7-431b-9c55-cba1cdaf93a4",
   "metadata": {},
   "source": [
    "DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afea6212-42e8-4663-bf88-c9e6c92271b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.2240, Validation Loss: 0.0741\n",
      "Epoch 2, Training Loss: 0.3169, Validation Loss: 0.0765\n",
      "Epoch 3, Training Loss: 0.2909, Validation Loss: 0.0544\n",
      "Epoch 4, Training Loss: 0.2675, Validation Loss: 0.0776\n",
      "Epoch 5, Training Loss: 0.2519, Validation Loss: 0.0509\n",
      "Epoch 6, Training Loss: 0.2452, Validation Loss: 0.0409\n",
      "Epoch 7, Training Loss: 0.2496, Validation Loss: 0.0623\n",
      "Epoch 8, Training Loss: 0.2327, Validation Loss: 0.0418\n",
      "Epoch 9, Training Loss: 0.2307, Validation Loss: 0.0395\n",
      "Epoch 10, Training Loss: 0.2223, Validation Loss: 0.0557\n",
      "Epoch 11, Training Loss: 0.2116, Validation Loss: 0.0411\n",
      "Epoch 12, Training Loss: 0.2154, Validation Loss: 0.0451\n",
      "Epoch 13, Training Loss: 0.2113, Validation Loss: 0.0402\n",
      "Epoch 14, Training Loss: 0.2063, Validation Loss: 0.0411\n",
      "Early stopping due to no improvement in validation loss.\n",
      "Training complete. Best model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalitha priya.a\\AppData\\Local\\Temp\\ipykernel_8356\\1782251258.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dnn_model.load_state_dict(torch.load('dnn_model_best.pth'))\n"
     ]
    }
   ],
   "source": [
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# Set up DNN model\n",
    "input_size = X_train_scaled.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dnn_model = DNNModel(input_size, output_size).to(device)\n",
    "\n",
    "# Define loss and optimizer with weight decay for L2 regularization\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dnn_model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "# Learning rate scheduler for dynamic adjustment\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# Prepare data loaders\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    dnn_model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = dnn_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation loss for early stopping\n",
    "    dnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = dnn_model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Adjust learning rate with scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(dnn_model.state_dict(), 'dnn_model_best.pth')  # Save best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "# Load the best model for evaluation\n",
    "dnn_model.load_state_dict(torch.load('dnn_model_best.pth'))\n",
    "print(\"Training complete. Best model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda199e3-8a6a-45a9-ac6c-19364e8d303d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Percentage Error (MPE): 4.13%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dnn_model.eval()\n",
    "\n",
    "# Forward pass to get predictions on the validation set\n",
    "with torch.no_grad():\n",
    "    val_predictions = dnn_model(X_val_tensor).cpu().numpy()  # Convert to numpy for calculation\n",
    "    y_val_true = y_val_tensor.cpu().numpy()\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    percentage_errors = (y_true - y_pred) / y_true * 100  # Calculate percentage errors\n",
    "    return np.mean(np.abs(percentage_errors))  # Return the mean of absolute percentage errors\n",
    "\n",
    "mpe = mean_percentage_error(y_val_true, val_predictions)\n",
    "print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f18a3f10-4a96-4f79-b939-9bad7dbf44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "dnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_dnn = dnn_model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "rmse_dnn = root_mean_squared_error(y_test, y_pred_dnn)\n",
    "mae_dnn = mean_absolute_error(y_test, y_pred_dnn)\n",
    "r2_dnn = r2_score(y_test, y_pred_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f75e665e-1c20-4613-a4a0-7b43d9b42046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN model saved as 'dnn_model.json'.\n"
     ]
    }
   ],
   "source": [
    "dnn_model_params = {k: v.cpu().tolist() for k, v in dnn_model.state_dict().items()}\n",
    "with open('dnn_model.json', 'w') as json_file:\n",
    "    json.dump(dnn_model_params, json_file)\n",
    "print(\"DNN model saved as 'dnn_model.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5765f396-e898-45b0-9c1e-f50ea6d49c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d757779-eb51-4d39-887b-dce67b5506b3",
   "metadata": {},
   "source": [
    "Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be75de4-f147-4bec-b5b6-01d79295978b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-learner parameters saved as 'meta_learner.json'.\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = xgb_reg.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "y_pred_combined = np.vstack((xgb_pred, y_pred_dnn.flatten())).T\n",
    "meta_learner = RandomForestRegressor()\n",
    "meta_learner.fit(y_pred_combined,y_test)\n",
    "# Convert meta-learner parameters to dictionary before saving\n",
    "meta_learner_params = meta_learner.get_params()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save parameters to JSON\n",
    "with open('meta_learner.json', 'w') as json_file:\n",
    "    json.dump(meta_learner_params, json_file)\n",
    "print(\"Meta-learner parameters saved as 'meta_learner.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad493ea9-0468-46f9-aad9-615c21218097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-learner saved as 'meta_learner.pkl'.\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(meta_learner, 'meta_learner.pkl')\n",
    "print(\"Meta-learner saved as 'meta_learner.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f555fbc4-d6b6-4509-b0ea-293cf7b29049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "Mean Percentage Error (MPE) for Meta-Learner: 1.03%\n"
     ]
    }
   ],
   "source": [
    "loaded_meta_learner = joblib.load('meta_learner.pkl')\n",
    "print(\"i\")\n",
    "y_pred_meta = loaded_meta_learner.predict(y_pred_combined)\n",
    "\n",
    "percentage_error_meta = 100 * np.abs((y_test - y_pred_meta) / y_test)\n",
    "mean_percentage_error_meta = np.mean(percentage_error_meta)\n",
    "print(f\"Mean Percentage Error (MPE) for Meta-Learner: {mean_percentage_error_meta:.2f}%\")\n",
    "\n",
    "rmse_meta = root_mean_squared_error(y_test, y_pred_meta)\n",
    "mae_meta = mean_absolute_error(y_test, y_pred_meta)\n",
    "r2_meta = r2_score(y_test, y_pred_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f35ac348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Mean MAE: 0.9920\n",
      "5-Fold Cross-Validation MAE Standard Deviation: 0.0004\n"
     ]
    }
   ],
   "source": [
    "k = 5  \n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "scoring = root_mean_squared_error(y_test, y_pred_meta)\n",
    "\n",
    "\n",
    "scores = cross_val_score(meta_learner, y_pred_combined, y_test, cv=kf,)\n",
    "\n",
    "\n",
    "mean_mae = np.mean(scores)\n",
    "std_mae = np.std(scores)\n",
    "\n",
    "print(f\"{k}-Fold Cross-Validation Mean MAE: {mean_mae:.4f}\")\n",
    "print(f\"{k}-Fold Cross-Validation MAE Standard Deviation: {std_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "572d43c7-e4af-4720-adc1-fd92bebba133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation completed. Results saved to 'model_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'Model': ['XGBoost', 'DNN', 'Meta-Learner'],\n",
    "    'RMSE': [rmse_xgb, rmse_dnn, rmse_meta],\n",
    "    'MAE': [mae_xgb, mae_dnn, mae_meta],\n",
    "    'R2 Score': [r2_xgb, r2_dnn, r2_meta]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "\n",
    "print(\"Model evaluation completed. Results saved to 'model_results.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d96ea-c759-420b-8360-34381722333d",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
